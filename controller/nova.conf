[DEFAULT]
#quota_cores=25
reserved_host_memory_mb = 0
ram_allocation_ratio = 1.0
cpu_allocation_ratio=2.0
verbose=True
auth_strategy=keystone
allow_resize_to_same_host=True
api_paste_config=/etc/nova/api-paste.ini
rootwrap_config=/etc/nova/rootwrap.conf
scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler
#compute_scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler
#scheduler_available_filters=nova.scheduler.filters.all_filters

# THIS FILTER IS ONLY FOR REGIONS W/ PCI PASSTHROUGH!!
#scheduler_available_filters=nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter
scheduler_default_filters=RamFilter,ComputeFilter,AvailabilityZoneFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter
#scheduler_default_filters=RamFilter,ComputeFilter,AvailabilityZoneFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,PciPassthroughFilter
dhcpbridge_flagfile=/etc/nova/nova.conf
force_dhcp_release=True
#fixed_range=10.0.0.0/24
s3_host=CTRL_IP
s3_port=3333
#osapi_compute_extension=nova.api.openstack.compute.contrib.standard_extensions
osapi_compute_listen = 0.0.0.0
metadata_listen = 0.0.0.0
my_ip=CTRL_IP
instance_name_template=instance-%08x
image_service=nova.image.glance.GlanceImageService
host=vc-edge-1
enabled_apis=osapi_compute,metadata
#volume_api_class=nova.volume.cinder.API
cinder_endpoint_template=http://CTRL_IP:8776/v1/%(project_id)s
state_path=/opt/stack/data/nova
instances_path=/opt/stack/data/nova/instances
logging_context_format_string=%(asctime)s %(color)s%(levelname)s %(name)s [[01;36m%(request_id)s [00;36m%(user_name)s %(project_name)s%(color)s] [01;35m%(instance)s%(color)s%(message)s[00m
logging_default_format_string=%(asctime)s %(color)s%(levelname)s %(name)s [[00;36m-%(color)s] [01;35m%(instance)s%(color)s%(message)s[00m
logging_debug_format_suffix=[00;33mfrom (pid=%(process)d) %(funcName)s %(pathname)s:%(lineno)d[00m
logging_exception_prefix=%(color)s%(asctime)s TRACE %(name)s [01;35m%(instance)s[00m
#network_api_class=nova.network.quantumv2.api.API
use_neutron = True
#linuxnet_ovs_integration_bridge=br-int
ec2_dmz_host=CTRL_IP
#rabbit_host=localhost
#rabbit_password=supersecret
transport_url = rabbit://guest:supersecret@CTRL_IP:5672/
compute_driver=libvirt.LibvirtDriver
#compute_driver=ironic.nova.virt.ironic.IronicDriver
firewall_driver = nova.virt.firewall.NoopFirewallDriver
resume_guests_state_on_host_boot=true
use_cow_images=true
running_deleted_instance_action=reap
running_deleted_instance_poll_interval=5
running_deleted_instance_timeout=120
rescue_timeout=0
reboot_timeout=0
instance_usage_audit_period=hour
instance_usage_audit=True

metadata_workers=2
osapi_compute_workers=2
#notification_driver=nova.openstack.common.notifier.rpc_notifier
notify_on_state_change=vm_and_task_state
#rpc_backend=nova.openstack.common.rpc.impl_kombu
#security_group_api=neutron
#vnc_enabled=true
debug=True
default_floating_pool=ext-net
lock_path=/opt/stack/data/nova
network_api_class=nova.network.neutronv2.api.API
#scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler
#scheduler_host_manager=ironic.nova.scheduler.ironic_host_manager.IronicHostManager
#compute_manager=ironic.nova.compute.manager.ClusteredComputeManager
#pci_alias={"vendor_id":"10de", "product_id":"1091", "name":"a1"}
vif_plugging_is_fatal=False
vif_plugging_timeout=0

[neutron]
service_metadata_proxy = True
url = http://CTRL_IP:9696
region_name = REGION_NAME
auth_strategy = keystone
#project_domain_name = Default
#user_domain_name = Default
project_name = service
password = supersecret
username = neutron
auth_url = http://iam.savitestbed.ca:5000/v2.0/
auth_type = password
service_metadata_proxy=True

# tlin upgrade notes: using deprecated values for now... above sections copied from newton
admin_auth_url = http://iam.savitestbed.ca:5000/v2.0/
admin_password = supersecret
admin_tenant_name = service
admin_username = neutron

[libvirt]
cpu_mode = none
virt_type = kvm
use_virtio_for_bridges=True
wait_soft_reboot_seconds=60
use_virtio_for_bridges=True
wait_soft_reboot_seconds=60

[vnc]
enabled = True
novncproxy_base_url=http://CTRL_IP:6080/vnc_auto.html
xvpvncproxy_base_url=http://CTRL_IP:6081/console
vncserver_listen=CTRL_IP
vncserver_proxyclient_address=CTRL_IP

[glance]
api_servers = http://CTRL_IP:9292
debug = True
use_glance_v1 = True

[database]
connection=mysql+pymysql://root:supersecret@CTRL_IP/nova?charset=utf8

[api_database]
connection=mysql+pymysql://root:supersecret@CTRL_IP/nova_api?charset=utf8

[osapi_v3]
enabled = False

[keystone_authtoken]
signing_dir = /var/cache/nova
admin_password = supersecret
admin_user = nova
admin_tenant_name = service
#auth_host = iam.savitestbed.ca
identity_uri = http://iam.savitestbed.ca:35357/

[spice]
enabled = false
html5proxy_base_url = http://CTRL_IP:6082/spice_auto.html


[ironic]
api_endpoint = http://CTRL_IP:6385/v1
project_name = service
auth_region = REGION_NAME
auth_url = http://iam.savitestbed.ca:35357/v2.0
password = ironicpass
username = ironic

[upgrade_levels]
# Set a version cap for messages sent to compute services. If
# you plan to do a live upgrade from havana to icehouse, you
# should set this option to "icehouse-compat" before beginning
# the live upgrade procedure. (string value)
#compute=kilo
#network=kilo
